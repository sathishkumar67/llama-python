{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightning\n!git clone https://github.com/sathishkumar67/llama-python.git\n!git clone https://huggingface.co/pt-sk/llama-py\n    \n# move the contents from /kaggle/working/llama-python to /kaggle/working/\n!mv /kaggle/working/llama-python/* /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:04:04.082149Z","iopub.execute_input":"2024-06-18T04:04:04.082691Z","iopub.status.idle":"2024-06-18T04:04:58.989931Z","shell.execute_reply.started":"2024-06-18T04:04:04.082657Z","shell.execute_reply":"2024-06-18T04:04:58.988665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport lightning as L\nfrom torch import optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nfrom architecture import ModelArgs, Transformer\nfrom dataset import DataArgs, TokenDataset\nfrom datasets import load_dataset\nfrom transformers import LlamaTokenizer\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ntorch.manual_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:04:58.992245Z","iopub.execute_input":"2024-06-18T04:04:58.993128Z","iopub.status.idle":"2024-06-18T04:05:07.527073Z","shell.execute_reply.started":"2024-06-18T04:04:58.993092Z","shell.execute_reply":"2024-06-18T04:05:07.526153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting the arguments\nargs = ModelArgs(\ndim = 768,\nn_layers = 12,\nn_heads = 12,\nvocab_size = 32000,\nmax_seq_len = 768,\nn_kv_heads = 6\n)\n\ndata_args = DataArgs(\n    batch_size = 8,\n    block_size = 768,\n    pad_token_id = 0\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:05:07.528501Z","iopub.execute_input":"2024-06-18T04:05:07.529062Z","iopub.status.idle":"2024-06-18T04:05:07.534498Z","shell.execute_reply.started":"2024-06-18T04:05:07.529028Z","shell.execute_reply":"2024-06-18T04:05:07.533611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bigcode/python-stack-v1-functions-filtered-sc2\n#muellerzr/python-stack-v1-functions-filtered-llama-3-8B\n#Vezora/Tested-22k-Python-Alpaca\n#Vezora/Tested-143k-Python-Alpaca","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:05:07.537494Z","iopub.execute_input":"2024-06-18T04:05:07.538433Z","iopub.status.idle":"2024-06-18T04:05:07.551359Z","shell.execute_reply.started":"2024-06-18T04:05:07.538399Z","shell.execute_reply":"2024-06-18T04:05:07.550508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the dataset\nds = load_dataset(\"Vezora/Tested-143k-Python-Alpaca\")\n\n# changing the format of the dataset\nds.set_format(type=\"pandas\")\n\n# taking the train split\ndf = ds[\"train\"][:]\n\n# adding text\ndf[\"text\"] = df[\"input\"] + \". \" + df[\"instruction\"] + \". \" + df[\"output\"]\n\n# df[\"text\"] = df[\"prompt\"] + \". \" + df[\"chosen\"]\ntext = \". \".join(df[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:05:07.552599Z","iopub.execute_input":"2024-06-18T04:05:07.552966Z","iopub.status.idle":"2024-06-18T04:05:16.598890Z","shell.execute_reply.started":"2024-06-18T04:05:07.552935Z","shell.execute_reply":"2024-06-18T04:05:16.597839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the tokenizer\ntokenizer = LlamaTokenizer(vocab_file=\"/kaggle/working/tokenizer.model\", add_bos_token=False, add_eos_token=False, legacy=False)\n\n# tokenizing the text\ntokens = tokenizer(text).input_ids","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:05:16.600148Z","iopub.execute_input":"2024-06-18T04:05:16.600463Z","iopub.status.idle":"2024-06-18T04:20:57.615333Z","shell.execute_reply.started":"2024-06-18T04:05:16.600438Z","shell.execute_reply":"2024-06-18T04:20:57.614225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preparing the dataset and dataloader\ndataset = TokenDataset(tokens, args=data_args)\ndataloader = DataLoader(dataset, batch_size=data_args.batch_size, drop_last=True, shuffle=True, num_workers=4)\n\nprint(f\"No of items in the dataloader: {len(dataloader)}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:20:57.616627Z","iopub.execute_input":"2024-06-18T04:20:57.616919Z","iopub.status.idle":"2024-06-18T04:20:57.623074Z","shell.execute_reply.started":"2024-06-18T04:20:57.616896Z","shell.execute_reply":"2024-06-18T04:20:57.622196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# activating the model\nmodel = Transformer(args).activate()","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:20:57.624174Z","iopub.execute_input":"2024-06-18T04:20:57.624532Z","iopub.status.idle":"2024-06-18T04:20:59.481239Z","shell.execute_reply.started":"2024-06-18T04:20:57.624501Z","shell.execute_reply":"2024-06-18T04:20:59.480189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# move the contents from /kaggle/working/llama_python/python-stack-v1-functions-filtered-llama-3-8B/version_1/checkpoints to /kaggle/working/\n# !mv /kaggle/working/llama_python/python-stack-v1-functions-filtered-sc2-stage-2/version_3/checkpoints/* /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:20:59.482457Z","iopub.execute_input":"2024-06-18T04:20:59.482751Z","iopub.status.idle":"2024-06-18T04:20:59.487006Z","shell.execute_reply.started":"2024-06-18T04:20:59.482726Z","shell.execute_reply":"2024-06-18T04:20:59.486004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Llama(L.LightningModule):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.learning_rates = []\n    \n    def training_step(self, batch, batch_idx):\n        logits = self.model(batch)\n        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), batch.view(-1), ignore_index=-1)\n        self.log(\"Loss\", loss, prog_bar=True)\n        return loss\n    \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.model.parameters(), lr=6e-4)\n        scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=6e-6)\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n    \n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step()\n        \n    def on_train_batch_end(self, outputs, batch, batch_idx):\n        # Log learning rate\n        optimizer = self.optimizers()\n        lr = optimizer.param_groups[0]['lr']\n        self.learning_rates.append(lr)\n    \n# checkpoint path\ncheckpoint_path = \"/kaggle/working/llama-py/llama-py_stage2.ckpt\"\n    \n# initialize the model\nllama = Llama.load_from_checkpoint(checkpoint_path, model=model)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:20:59.489652Z","iopub.execute_input":"2024-06-18T04:20:59.489951Z","iopub.status.idle":"2024-06-18T04:21:00.875245Z","shell.execute_reply.started":"2024-06-18T04:20:59.489927Z","shell.execute_reply":"2024-06-18T04:21:00.874409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n\nprint(f\"Total parameters: {count_parameters(llama.model)}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:21:00.876318Z","iopub.execute_input":"2024-06-18T04:21:00.876599Z","iopub.status.idle":"2024-06-18T04:21:00.882946Z","shell.execute_reply.started":"2024-06-18T04:21:00.876576Z","shell.execute_reply":"2024-06-18T04:21:00.882002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete all the directories and files in /kaggle/working/\n# !rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:21:00.884144Z","iopub.execute_input":"2024-06-18T04:21:00.884444Z","iopub.status.idle":"2024-06-18T04:21:00.894189Z","shell.execute_reply.started":"2024-06-18T04:21:00.884421Z","shell.execute_reply":"2024-06-18T04:21:00.893223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = L.Trainer(max_epochs=1, accelerator=\"cuda\")\ntrainer.fit(llama, dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:21:00.895335Z","iopub.execute_input":"2024-06-18T04:21:00.895636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %reload_ext tensorboard\n# %tensorboard --logdir=lightning_logs/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Experiment \n\n* The code was shorter than the one used in mamba, cloned the files and directly imported files from the respective scripts\n* The code was more organized and easier to understand, with clear comments and function names\n* used arguments for data and model\n* codes in the transformer is effiecient but make even efficient\n* used pretrained llama tokenizer\n* used torch.manual_seed(42)\n* arguments are implicitly given, future idea is to give the arguments using json file format loading the model using that configuration\n* directly combined the text without using any seperation variables\n* eos or sos can be integrated\n* samples are shuffled and the last batch is ignored\n* dataloader can be improved by using shuffle=False, droplast=False, ignoring only the last item not last batch\n* activated the model and the size of the model is 254M\n* using lightning module as a base class for our model\n* with the help of lightningmodule the train loop is so easy, efficient in both memory and time. mainl a lot of memory is saved\n* used only registering loss in the progress bar\n* used adaw optimizer without any change in parameter values and no schedules are used and the learning rate is 1e-4. may other techniques or optimizer can be used like schedulefree optimizer\n* in future training many techniques are there. those techniques can be used\n* did not find how to store the model\n* started training using only 1gpu \"p100\" for 1epoch\n* training:\n    * started training using muellerzr/python-stack-v1-functions-filtered-llama-3-8B dataset for 2epochs with shuffle for 2 epochs\n    * started training using bigcode/python-stack-v1-functions-filtered-sc2 dataset for 2epochs with shuffle for 2 epochs\n    * started training using Vezora/Tested-22k-Python-Alpaca dataset for 2epochs with shuffle for 2 epochs\n    * started training using Vezora/Tested-143k-Python-Alpaca dataset for 2epochs with shuffle for 1 epochs","metadata":{}},{"cell_type":"code","source":"# from huggingface_hub import login\n# login()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T12:58:55.623751Z","iopub.execute_input":"2024-06-17T12:58:55.624131Z","iopub.status.idle":"2024-06-17T12:58:55.645081Z","shell.execute_reply.started":"2024-06-17T12:58:55.624102Z","shell.execute_reply":"2024-06-17T12:58:55.644180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # push entire folder to the hub\n# from huggingface_hub import HfApi\n# api = HfApi()\n\n# api.upload_folder(\n#     folder_path=\"/kaggle/working/lightning_logs\",\n#     path_in_repo=\"python-stack-v1-functions-filtered-sc2-stage-2\",\n#     repo_id=\"pt-sk/llama_python\",\n#     repo_type=\"model\",\n# )","metadata":{"execution":{"iopub.status.busy":"2024-06-17T14:55:38.032395Z","iopub.execute_input":"2024-06-17T14:55:38.033239Z","iopub.status.idle":"2024-06-17T14:57:09.278974Z","shell.execute_reply.started":"2024-06-17T14:55:38.033202Z","shell.execute_reply":"2024-06-17T14:57:09.278026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # clear ram memory\n# import gc\n# del llama\n# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T14:57:28.755801Z","iopub.execute_input":"2024-06-17T14:57:28.756550Z","iopub.status.idle":"2024-06-17T14:57:31.600022Z","shell.execute_reply.started":"2024-06-17T14:57:28.756521Z","shell.execute_reply":"2024-06-17T14:57:31.598672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}